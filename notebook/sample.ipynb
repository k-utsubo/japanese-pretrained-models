{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "https://github.com/rinnakk/japanese-pretrained-models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "from transformers import T5Tokenizer, RobertaForMaskedLM\r\n",
    "\r\n",
    "# load tokenizer\r\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-roberta-base\")\r\n",
    "tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\r\n",
    "\r\n",
    "# load model\r\n",
    "model = RobertaForMaskedLM.from_pretrained(\"rinna/japanese-roberta-base\")\r\n",
    "model = model.eval()\r\n",
    "\r\n",
    "# original text\r\n",
    "text = \"4年に1度オリンピックは開かれる。\"\r\n",
    "\r\n",
    "# prepend [CLS]\r\n",
    "text = \"[CLS]\" + text\r\n",
    "\r\n",
    "# tokenize\r\n",
    "tokens = tokenizer.tokenize(text)\r\n",
    "print(tokens)  # output: ['[CLS]', '▁4', '年に', '1', '度', 'オリンピック', 'は', '開かれる', '。']']\r\n",
    "\r\n",
    "# mask a token\r\n",
    "masked_idx = 5\r\n",
    "tokens[masked_idx] = tokenizer.mask_token\r\n",
    "print(tokens)  # output: ['[CLS]', '▁4', '年に', '1', '度', '[MASK]', 'は', '開かれる', '。']\r\n",
    "\r\n",
    "# convert to ids\r\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
    "print(token_ids)  # output: [4, 1602, 44, 24, 368, 6, 11, 21583, 8]\r\n",
    "\r\n",
    "# convert to tensor\r\n",
    "token_tensor = torch.tensor([token_ids])\r\n",
    "\r\n",
    "# get the top 10 predictions of the masked token\r\n",
    "with torch.no_grad():\r\n",
    "    outputs = model(token_tensor)\r\n",
    "    predictions = outputs[0][0, masked_idx].topk(10)\r\n",
    "\r\n",
    "for i, index_t in enumerate(predictions.indices):\r\n",
    "    index = index_t.item()\r\n",
    "    token = tokenizer.convert_ids_to_tokens([index])[0]\r\n",
    "    print(i, token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 806k/806k [00:00<00:00, 15.2MB/s]\n",
      "Downloading: 100%|██████████| 153/153 [00:00<00:00, 37.4kB/s]\n",
      "Downloading: 100%|██████████| 259/259 [00:00<00:00, 263kB/s]\n",
      "Downloading: 100%|██████████| 663/663 [00:00<00:00, 332kB/s]\n",
      "Downloading: 100%|██████████| 443M/443M [00:10<00:00, 43.4MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['[CLS]', '▁4', '年に', '1', '度', 'オリンピック', 'は', '開かれる', '。']\n",
      "['[CLS]', '▁4', '年に', '1', '度', '[MASK]', 'は', '開かれる', '。']\n",
      "[4, 1602, 44, 24, 368, 6, 11, 21583, 8]\n",
      "0 ワールドカップ\n",
      "1 フェスティバル\n",
      "2 オリンピック\n",
      "3 サミット\n",
      "4 東京オリンピック\n",
      "5 総会\n",
      "6 全国大会\n",
      "7 イベント\n",
      "8 世界選手権\n",
      "9 パーティー\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from transformers import T5Tokenizer, AutoModelForCausalLM\r\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\r\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\r\n",
    "tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\r\n",
    "\r\n",
    "model = AutoModelForCausalLM.from_pretrained(\"rinna/japanese-gpt2-medium\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "inputs=tokenizer(\"こんにちは，世界！\")\r\n",
    "print(inputs)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [9, 30442, 11, 83, 301, 543, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "outputs = model.generate(input_ids=torch.tensor([inputs[\"input_ids\"]]))\r\n",
    "text = tokenizer.decode(\r\n",
    "    outputs[0].tolist(),\r\n",
    "    skip_special_tokens=True,\r\n",
    "    clean_up_tokenization_spaces=True,\r\n",
    ")\r\n",
    "print(text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "こんにちは,世界!ニッポンの達人です。 今回は,「日本は\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}